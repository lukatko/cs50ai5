First of all, I tried changing some parameters of max pooling and convolutional layers which didn't really work. Then I added some hidden layers. This step was really useful, as the model got significantly better. I found that an ideal number of hidden layers for this problem should be 4 with 128 neurons. In the next step I tried to find an optimal number of Convolutional layers. When I tried 3, the model didn't really improve, but with 2 conv layers, the model improved by approximately 1 percent. One MaxPool layer also improved the model and sped up the training. The best optimizer for this problem is "adam" I think. The best accuracy for the test set, which I could get, was 97.58 %. I used a dropout of 0.45. I think if we tweaked some parameters even more, we could get even 98% accuracy or more.